# vLLM configuration file
# This will be used for serving the LLM model via OpenAI-compatible API